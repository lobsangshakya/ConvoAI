# ConvoAI â€” Simple AI Chatbot

A beginner-friendly chatbot with a **React** frontend and **FastAPI** backend, powered by [Ollama](https://ollama.ai) for local AI inference.

![React](https://img.shields.io/badge/React-18-blue) ![FastAPI](https://img.shields.io/badge/FastAPI-Python-green) ![Ollama](https://img.shields.io/badge/Ollama-Local_AI-orange)

---

## What Does This Project Do?

You type a message â†’ the React frontend sends it to the FastAPI backend â†’ the backend forwards it to Ollama (a local AI model running on your machine) â†’ and streams the response back in real time, just like ChatGPT.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React   â”‚ â”€â”€â”€â–¶  â”‚  FastAPI  â”‚ â”€â”€â”€â–¶  â”‚  Ollama  â”‚
â”‚ Frontend â”‚ â—€â”€â”€â”€  â”‚  Backend  â”‚ â—€â”€â”€â”€  â”‚  (AI)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 localhost:3000     localhost:8000     localhost:11434
```

---

## Prerequisites

Make sure you have these installed:

| Tool       | Version | How to install                          |
| ---------- | ------- | --------------------------------------- |
| **Node.js** | 18+     | [nodejs.org](https://nodejs.org)       |
| **Python**  | 3.9+    | [python.org](https://python.org)       |
| **Ollama**  | latest  | [ollama.ai](https://ollama.ai)         |

---

## Quick Start (3 steps)

### 1. Install Ollama and pull a model

```bash
# After installing Ollama from ollama.ai, pull a model:
ollama pull qwen2.5:3b

# Start the Ollama server (it may already be running):
ollama serve
```

### 2. Start the Backend

```bash
# Open a terminal and run:
cd backend
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000
```

You should see: `Uvicorn running on http://127.0.0.1:8000`

### 3. Start the Frontend

```bash
# Open a NEW terminal and run:
cd frontend
npm install
npm start
```

Your browser will open `http://localhost:3000` â€” start chatting! ğŸ‰

---

## Project Structure

```
ConvoAI/
â”œâ”€â”€ backend/                 # Python FastAPI server
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py          # API endpoints (/api/chat, /api/health)
â”‚   â”‚   â”œâ”€â”€ ollama_provider.py  # Talks to Ollama
â”‚   â”‚   â””â”€â”€ rag_service.py   # (Optional) RAG for knowledge-base Q&A
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”‚
â”œâ”€â”€ frontend/                # React chat interface
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js           # Main chat component
â”‚   â”‚   â”œâ”€â”€ App.css          # Styles
â”‚   â”‚   â””â”€â”€ index.js         # React entry point
â”‚   â”œâ”€â”€ public/index.html
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ knowledge/               # (Optional) Drop .txt/.md files here for RAG
â”‚   â””â”€â”€ sample.txt
â”‚
â”œâ”€â”€ .env                     # Your local config (not committed to git)
â”œâ”€â”€ .env.example             # Template â€” copy this to .env
â””â”€â”€ README.md                # You are here!
```

---

## Configuration

Copy `.env.example` to `.env` and adjust if needed:

```bash
cp .env.example .env
```

| Variable             | Default                       | Description                        |
| -------------------- | ----------------------------- | ---------------------------------- |
| `OLLAMA_BASE_URL`    | `http://127.0.0.1:11434`     | Where Ollama is running            |
| `OLLAMA_MODEL`       | `qwen2.5:3b`                 | Which model to use                 |
| `OLLAMA_TIMEOUT`     | `60`                          | Request timeout in seconds         |
| `ENABLE_RAG`         | `0`                           | Set to `1` to enable RAG           |
| `REACT_APP_BACKEND_URL` | `http://localhost:8000`    | Backend URL for the frontend       |

---

## API Endpoints

| Method | Endpoint           | Description                    |
| ------ | ------------------ | ------------------------------ |
| POST   | `/api/chat`        | Send a message, get a reply    |
| POST   | `/api/chat/stream` | Send a message, get a streamed reply |
| GET    | `/api/health`      | Check if the server is running |

**Example request:**

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!"}'
```

---

## Optional: RAG (Knowledge Base)

Want the bot to answer questions using your own documents?

1. Drop `.txt` or `.md` files into the `knowledge/` folder
2. Set `ENABLE_RAG=1` in your `.env` file
3. Restart the backend

The bot will automatically read and index your documents on startup.

---

## Troubleshooting

| Problem                          | Solution                                            |
| -------------------------------- | --------------------------------------------------- |
| "Ollama is not available"       | Make sure Ollama is running: `ollama serve`          |
| "Failed to fetch" in frontend   | Make sure the backend is running on port 8000        |
| Slow first response             | First response is slower while the model loads       |
| "Model not found"               | Pull the model first: `ollama pull qwen2.5:3b`      |

---

## License

MIT â€” see [LICENSE](./LICENSE) for details.